# Introduction
최초 작성일 : 2024-01-22  
마지막 수정일 : 2024-01-26
  
## 0. Overview

이 글에서는 빅데이터에 대한 개념과 구성에 대해서 설명한다. 참고한 도서의 설명과 시각적 자료를 기반으로, 설명이 부족하다고 생각되는 부분을 추가적인 조사를 통해 내용을 구성했다. 또, 추가적인 설명이 요구되는 용어에 대해서는 별도의 문서를 통해 정리하였다.

 [Terminology.md](https://github.com/seminarNotes/engineering--Smartcar-real-time-log-processing/blob/main/Terminology.md)


## Table of Contents

1. [Big Data Definition](#1.-Definition-of-Big-Data)
2. [Big Data Objectives](#2.-Big-Data-Objectives)
3. [Big Data Utilization](#3.-Utilization-of-Big-Data)
4. [Big Data Project](#4.-Definition-of-Big-Data)
5. [Big Data Technology Evolution](#5.-Big-Data-Technology-Evolution)
6. [Big Data Implementation Technology](#6.-Big-Data-Implementation-Technology)
7. [Big Data Role and Responsibilities](#7.-Big-Data-Role-and-Responsibilities)
8. [Big Data Security](#8.-Big-Data-Security)


## 1. Definition of Big Data

빅데이터를 단순히 큰 데이터, 큰 정보라만 보지 않는다. 과거부터 현재까지 쌓인 데이터를 분석해 현재를 이해한다. 데이터에서 만들어 다양한 패턴을 해석해 미래를 예측하기 시작하였다.

빅데이터는 다음 6V로 정의하며, 6V는 빅데이터와 관련되어 중요한 키워드로 자리잡고 있다.

- Volume (양): 빅데이터는 대량의 데이터를 다루는 것
- Velocity (속도): 데이터가 빠른 속도로 생성되고 전달되는 특성
- Variety (다양성): 다양한 종류의 데이터 유형과 형식을 포함
- Veracity (정확성): 데이터의 정확성과 신뢰성
- Value (가치): 데이터로부터 가치를 추출하고 활용하는 능력
- Variability (변동성): 데이터의 변동성 및 불규칙성을 다루며, 데이터의 빠른 변화

<img src="./images/introduction_big_data_definition.png" width="600" height="300" alt="introduction_big_data_definition">

이러한 6가지 "V"는 빅데이터의 주요 특성을 설명하는데 도움을 주며, 빅데이터 분석 및 활용에 대한 방향성을 제시하는데 사용한다. 결론적으로, 위 6V를 통해 보편적으로 빅데이터는 


*현재 이 순간에도 방대한 크기(Volume)의 다양한(Varity) 데이터들이 빠른 속도(Velocity)로 발생하고 있다. 빅데이터는 3V(Volume, Varity, Velocity)를 수용하며, 데이터의 진실성(Veracity)을 확보하고, 분석 데이터를 시각화(Visualization)함으로써 새로운 효익을 가져다 줄 가치(Value)를 창출하는 것*


으로 정의된다.

## 2. Big Data Objectives
빅데이터의 목적은 무엇인가? 빅데이터를 통해 어떤 장점을 활용할 수 있나? 빅데이터를 활용하기 위해서는 값비싼 하드웨어와 소프트웨어, 전문인력을 사용하기 때문에 시스템 구축시 큰 비용이 발생하며, 복잡한 시스템으로 인해 많은 유지보수 비용이 따른다. 따라서, 빅데이터 시스템을 구축하고 운영하기 위해서는 목적과 예상되는 payoff를 맞게 설계 해야 한다. 빅데이터를 활용하는 목적에 따라 설계되는 규모가 어느 정도 결정되니, 결과적으로는 빅데이터의 최초 도입 및 재구축 시 빅데이터 시스템의 구축 목적을 확실히 짚고 넘어가야 한다. 

<img src="./images/introduction_big_data_objectives1.png" width="600" height="300" alt="introduction_big_data_objectives1">

위 그림에서 말하는 것은 성공적인 빅데이터의 시스템의 구축과 운영을 위해, 데이터, 사람, 기술을 고려해야한다는 것이다. 또한, 빅데이터 시스템을 통해 정보를 제공하여, 최종적으로, 다음 3가지와 같은 의사결정 인사이트를 발견하는데 그 목적이 있다.

- 비용 절감 인사이트
- 수익 창출 인사이트
- 문제 해결 인사이트


그렇다면, 빅데이터의 인사이트는 무엇일까 ? 빅데이터의 인사이트는 통찰력이라고 하며, 현상을 이해하는 인사이트, 현상을 발견하는 인사이트, 현상을 예측하는 인사이트로 나뉜다. 현상의 이해는 대규모 데이터로부터 통계량을 산출하고, 과거에 발생한 일에 대한 원인을 분석하는 것이다. 현상의 발견은 데이터의 패턴을 발견하고,해석하고, 해석을 통해 현재 발생하는 일을 분석하는 인사이트라 할 수 있다. 마지막으로, 현상 예측 인사이트는 현재 발생하고 있는 일을 모형에 적용하고, 미래에 발생할 현상을 예측하는 인사이트이다. 또한, 인사이트를 발견하면 비용 절감, 수익 창출, 문제 해결이라는 빅데이터의 최종 목적을 달성하게 된다. 따라서, 빅데이터의 목적을 분명히 하여, 문제를 해결하기 위한 목적보다 기술을 우선시하는 상황이 발생하지 않도록 해야 한다.

<img src="./images/introduction_big_data_objectives2.png" width="600" height="750" alt="introduction_big_data_objectives2">

## 3. Big Data Utilization
빅데이터는 비용 절감, 수익 창출, 문제 해결의 목적을 위해 활용한다는 사실을 분명히 하였다. 그렇다면 어떻게(HOW) 활용해야 할까? 현재 빅데이터를 사용하는 사업 분야는 너무나 다양하지만, 넓은 영역에서 다음 3가지 방법으로 활용된다고 볼 수 있다.

- 상품/서비스 : 빅데이터를 상품/서비스 개발 및 개선에 활용
- 마케팅 지원 : 빅데이터를 대규모 고객 및 시장 분석에 활용
- 리스크 관리 : 빅데이터를 리스크 검출 및 예측 분석에 활용

<img src="./images/introduction_big_data_utilization1.png" width="600" height="300" alt="introduction_big_data_utilization1">

위 그림에서 3V 데이터를 빅데이터 레이크(Big data Lake)에 저장하고,


<img src="./images/introduction_big_data_utilization2.png" width="600" height="300" alt="introduction_big_data_utilization2">


## 4. Big Data Project
빅데이터는 크게 다음 3가지의 유형이 존재한다.

- 플랫폼 구축형 프로젝트
- 빅데이터 분석 프로젝트
- 빅데이터 운영 프로젝트


### 4.1. Platform Development Project
전형적인 빅데이터 SI(System Integram)를 구축하는 사업을 의미하며, 빅데이터의 하드웨어와 소프트웨어를 설치하고 구현함으로써, 수집, 적재, 처리, 탐색, 분석과 같은 일련의 과정을 구현하는 것을 의미한다. 규모에 따라 차이가 나지만, 대게 3~6개월 정도로 프로젝트 기간이 계획된다. 또한, 데이터 저장소가 설계되고 난 후, 다른 저장소에 있는 데이터를 migration하는 작업도 포함될 수 있다.

### 4.2. Big Data Analysis Project
빅데이터 플랫폼 구축 완료 후 수행이 되는 프로젝트로, 빅데이터 탐색으로 데이터의 이해가 높아 질때, 시작한다. 약 1~3개월 일정으로 추친되고, 분석주제영역은 마케팅/고객 분석, 상품/서비스 개발, 리스크 관리를 주제로 프로젝트가 진행된다.

### 4.3. Big Data Operations Project
구축 완료된 플랫폼을 중장기적으로 유지 관리하는 것을 의미한다. 대규모 하드웨어/소프트웨어로 운영 비용이 높다는 것이 특징이다. 대규모 하드웨어/소프트웨어/네트워크가 분산 환경으로 구성되어 있고, 많은 에러와 이슈들이 발생한다. 또한, 오픈 소스 프레임워크들의 업데이터와 기술 발전 속도가 빠르기 때문에 업그레이드와 패치가 빈번히 발생한다. 이러한 이유로, 빅데이터 분야별 전문가 그룹이 확보되는 것이 요구된다.

## 5. Big Data Technology Evolution
초기에는 빅데이터를 Storage(저장소)의 역할로만 기술이 발전이 되었지만, 이후, 이머징 기술이 따라 빠른 발전를 맞게 되었다. 빅데이터의 기술 변화는 아래 그림과 같다.

<img src="./images/introduction_big_data_technology_evolution.png" width="500" height="250" alt="introduction_big_data_technology_evolution">

초기에는 대용량 저장소와 대규모 배치 처리 기술이 집중이 되었다. 이 후, 실시간 처리와 온라인 분석 기술이 발전이 되었고, 현재는 전처리 및 분석 마트, 고급 분석 및 마이닝에 초점이 맞춰 발전하고 있다. 특히, 빅데이터 기술의 중심에는 Hadoop이라는 프레임워크가 존재한다.


## 6. Big Data Implementation Technology
빅데이터를 처리하는 단계는 수집-적재-처리-탐색-분석-응용이며, 이 단계를 처리하기 위한 기술과 프레임워크가 매우 다양하다.

<img src="./images/introduction_big_data_implementation-technology1.png" width="500" height="250" alt="introduction_big_data_implementation-technology1">

### 6.1. Collection  
빅데이터의 수집 기술은 조직의 내/외부의 다양한 시스템으로부터 원천 데이터를 효과적으로 수집하는 기술이다. 빅데이터 수집에는 기존의 수집 시스템에서 다뤘던 데이터보다 더 크고 다양한 형식의 데이터를 빠르게 처리해야 하기 때문에 분산 기능의 선형적 확장이 필요한 것이 특징이다. 빅데이터의 수집기는 원천 시스템의 다양한 인터페이스 유형, 데이터베이스, 파일, API, 메세지 등과 연결되어 정형/비정형 데이터를 대용량으로 수집한다. 특히, 외부데이터 소셜 미디어, 블로그, 포털, 뉴스 등의 데이터를 수집 크롤링과 NLP 등 비정형 처리를 위한 기술이 선택적으로 적용된다.

#### 6.1.1. Structured Data Collection  
정형 데이터(Structured Data Collection)는 구조화된 형식을 가진 데이터로, 일반적으로 표 형태로 표현되며, 데이터베이스 테이블, 엑셀 스프레드시트가 대표적인 정형 데이터 포맷이다. 정형 데이터는 아래와 같은 방법을 통해 수집된다.

- 데이터베이스 쿼리: 관계형 데이터베이스에서 SQL 쿼리를 사용하여 데이터를 추출하고 수집
- 파일 수집: 정형 데이터 파일(.csv)을 수집하여 데이터를 추출
- 웹 스크레이핑: 웹 페이지에서 특정 데이터를 추출하는 기술로, 정형 데이터를 수집

#### 6.1.2. Unstructured Data Collection
비정형 데이터(Unstructured Data Collection)는 구조화되지 않은 데이터로, 텍스트, 이미지, 비디오, 소셜 미디어 게시물 등이 해당하며, 비정형 데이터는 아래와 같은 방법으로 수집된다.

- 텍스트 마이닝: 텍스트 데이터에서 의미 있는 정보를 추출하고 수집하기 위해 자연어 처리(NLP) 기술을 활용
- 이미지 및 비디오 분석: 이미지와 비디오에서 정보를 추출하고 수집하기 위해 컴퓨터 비전 및 영상 처리 기술을 사용
- 웹 크롤링: 웹에서 비정형 데이터를 추출하는 기술로, 웹사이트의 텍스트, 이미지, 링크 등을 수집

한편, 데이터 종류가 아닌 데이터 수집 처리 방식에 대해 대용량 파일 수집과 실시간 스트림 수집으로 나눌 수 있는데, 실시간 수집의 경우에는 CEP 컴플렉스 이벤트 프로세싱, ESP 이벤트 스트림 프로세스 기술이 적용되어 수집 중인 데이터로부터 이벤트를 감지해 빠른 후속 처리를 수행한다. 두 수집 방식에 대해 아래와 같이 정리할 수 있다.

#### 6.1.3. Large File Collection  
대용량 파일 수집(Large File Collection)은 크기가 큰 파일을 수집하고 저장하는 것을 의미한다. 정형 데이터와 비정형 데이터 모두 이러한 파일을 통해 데이터가 수집될 수 있다. 대용량 파일 수집에는 다음과 같은 방식을 사용한다

- 파일 전송 프로토콜 (FTP): FTP를 사용하여 원격 서버로부터 대용량 파일을 다운로드하거나 업로드
- 분산 파일 시스템: 대용량 파일을 저장하고 처리하기 위해 Hadoop HDFS, Amazon S3와 같은 분산 파일 시스템을 사용

#### 6.1.4. Real-time Stream Collection
실시간 스트림 수집(Real-time Stream Collection)은 데이터가 생성되는 즉시 데이터를 수집하는 것을 의미하며, 주로 이러한 데이터는 실시간 분석 및 대시보드에 활용된다. 실시간 스트림 수집에는 다음과 같은 방법이 활용된다.

- 메시징 시스템: Apache Kafka, RabbitMQ와 같은 메시징 시스템을 사용하여 스트림 데이터를 수집하고 처리
- 로그 수집: 서버 또는 응용 프로그램의 로그를 수집하여 실시간 분석 또는 모니터링에 사용
- 소셜 미디어 스트림: 트위터, 페이스북 등의 소셜 미디어 스트림에서 데이터를 수집하여 실시간 트렌드 분석 등에 사용


### 6.2. Storage  
수집한 데이터를 분산 스토리지에 영구/임시 저장하는 기술이다.  다양한 데이터 저장소 유형에 따라 데이터를 영구적으로 저장하거나 버퍼링하고 처리할 수 있으며, 아래와 같이 크게 4가지 유형으로 구분될 수 있다.

#### 6.2.1. Distributed Storage for Storing Entire Large Files  
대용량 파일 전체를 영구적으로 저장하기 위해 사용되는 데이터 저장소로, 파일의 크기가 크기 때문에 주로 분산 파일 시스템에 활용된다. 대표적으로 Hadoop HDFS, Amazon S3, Google Cloud Storage 등이 있다.

#### 6.2.2. NoSQL Database for Storing Entire Large-scale Messaging Data
NoSQL 데이터베이스는 대규모 메시징 데이터를 영구적으로 저장하며, Structure된 Schema가 없기 때문에 RDBMS보다 확장성이 뛰어나가는 장점이 있다. 대표적으로 MongoDB, Cassandra, HBase 등이 있다.

#### 6.2.3. In-Memory and Cache Storage for Storing Partial Large-scale Messaging Data
대규모 메시징 데이터 중 일부만을 메모리에 저장하여 빠른 액세스를 하기 위해 사용하며, 따라서, 실시간 처리 및 검색 기능을 위해 활용되는 것이 적절하다. Redis, Memcached, Apache Kafka가 이 역할을 수행할 수 있다.

#### 6.2.4. Message Oriented Middleware for Buffering Entire Large-scale Messaging Data
Message Oriented Middleware(i.e. MOM)는 대규모 메시징 데이터를 버퍼링하고 중간에 전달하며,  데이터 흐름을 관리하고 스케일링을 지원하는 미들웨어이다.  대표적인 예시로 Apache Kafka, RabbitMQ, ActiveMQ 등이 있다.

빅데이터 적재 기술은 수집된 데이터의 성격에 따라 적재 저장소를 구분하여 유형별로 저장해야 한다. 예를 들어, 대용량 파일 적재는 주로 HDFS 저장소를 사용하면 되지만, 실시간 및 대량으로 발생하는 작은 메세지 데이터를 HDFS에 저장할 경우, 타일 수가 기하급수적으로 증가하여, 관리 노드와 병렬 처리의 효율성이 크게 떨어진다. 빅데이터가 적재될때는 추가적으로 전처리 작업이 수행되기도 하는데, 탐색 분석 단계를 위한 비정형, 음성, 이미지, 텍스트, 동영상 데이터를 정형 데이터로 가공하거나 개인 정보로 판단되는 데이터를 비식별 처리하는 작업도 포함이 된다. 

### 6.3. Processing/Exploration 
빅데이터 처리/탐색 과정은 대용량 저장소에 적재된 데이터를 분석에 활용하기 우해 데이터를 정형화/정규화를 하는 기술이며, 아래와 같은 과정을 포함한다. 

#### 6.3.1. Data Cleansing  
데이터 정제(Data Cleansing)는 수집된 데이터 중에서 오류, 중복, 누락된 값 등을 식별하고 정제하여 데이터의 품질을 향상시키는 과정이다.

#### 6.3.2. Data Transformation   
데이터 변환(Data Transformation)는 데이터를 분석 가능한 형식으로 변환하거나, 데이터를 표준화하여 일관된 형태로 변환하는 것이다. 데이터의 스케일을 조정하거나, 필요한 속성을 추출하는 등의 작업이 이루어진다.

#### 6.3.3. Data Integration   
데이터 통합(Data Integration)은 여러 소스에서 수집한 데이터를 통합하여 단일 데이터 셋으로 통합(merge)한다. 데이터의 일관성을 유지하며 서로 다른 소스에서 가져온 데이터를 이용한다.

#### 6.3.4. Data Exploration   
데이터 탐색(Data Exploration)은 탐색적 데이터 분석(EDA)을 수행하여 데이터를 시각화하고 통계적인 방법으로 탐색이다. 데이터의 패턴, 관계, 이상치 등 통계적 가치를 발견하는 과정이다.

#### 6.3.5. Data Joining
데이터 결합(Data Joining)은 다양한 데이터 세트를 결합하거나 조인하여 새로운 데이터 세트를 생성한다. 이 과정을 통해 다른 소스에서 가져온 데이터 간의 관계를 파악할 수 있다. 이 때, 데이터 통합과 다른 점은, 데이터 통합은 여러 다른 데이터 소스에서 데이터를 수집하고 이를 통합하여 단일 데이터 세트로 만드는 과정을 나타내고, 데이터 결합은  주로 관계형 데이터베이스에서 사용되는 용어로, 여러 테이블 간의 데이터를 연결하거나 결합하는 프로세스를 의미한다.

#### 6.3.6. Data Reduction  
데이터 축소 (Data Reduction)는 필요한 데이터만을 추출하거나 샘플링하여 데이터 크기나 차원을 줄이는 것을 의미한다. 대용량 데이터를 처리 및 분석하기 위한 효율성을 높이고, 계산적 비용을 줄이는데 중요한 단계이다.

#### 6.3.6. Data Storage
데이터 저장(Data Storage)은 처리된 후 데이터는 지속적으로 저장되며, 데이터 마트, 데이터 웨어하우스, 또는 분산 스토리지 시스템 등에 데이터의 목적과 역할에 맞게 다시 보관한다. 최초 데이터가 입수 되고 저장되었을 때와 다른 점은, 해당 단계에서의 데이터는 품질적으로 한번 가공이 되었다는 것에 의미가 있다.

#### 6.3.7. Workflow Automation  
워크플로우 자동화(Workflow Automation)는 처리 및 탐색 과정을 자동화하여 정기적으로 실행될 수 있도록 한다. 대표적인 프레임워크로는 Apache Airflow, Apache NiFi, Oozie 등이 있다.

#### 6.3.8. Data Structuring  
데이터 구조화(Data Structuring)는 최종 결과 데이터는 특화된 데이터 저장소나 데이터 마트에 구조화하여 저장한. 이로써 데이터셋의 측정 가능한 구조가 만들어지며, 이 후의 과정인 빅데이터 분석을 빠르고 정확하게 수행할 수 있게 한다. 데이터 저장과 데이터 구조화의 차이는 데이터 저장은 데이터를 용도에 맞게  데이터 마트, 데이터 웨어하우스, 또는 분산 스토리지 시스템에 저장하는 과정이고, 데이터 구조화는 데이터 구조화는 저장된 데이터를 의미 있는 방식으로 조직하고 정리하는 과정을 의미한다.

### 6.4. Analysis/Application
분석 기술은 대규모 데이터로부터 새로운 패턴을 찾고, 그 패턴을 해석해서 통찰력을 확보하기 위한 기술이다. 빅데이터 분석은 활용 영역에 따라 통계, 데이터 마이닝, 텍스트 마이닝, 소셜미디어 분석, 머신러닝 분석, 딥러닝 분석 등으로 분류된다. 특히, 분산 환경 위에서 머신 러닝 기술을 구현해 군집, 분류, 회귀, 추천 등의 고급 분석 영역까지 확장할 수 있다.

## 7. Big Data Role and Responsibilities
R&R 혹은 Role and Responsibilities는 역할과 책임를 의미한다. 다시 말해, 기업 조직에서 개별 프로세스 및 조직의 구성원들이 수행해야할 '역할'과 그 역할의 수행에 따른 '책임' 관계를 뜻한다. 빅데이터 시스템과 AI 시스템(데이터 분석 시스템)은 대게 대규모로 구성되기 때문에 다양한 역할과 책임 그리고 관계 부서들로 이루어진 시스템으로 구성된다. 따라서, 효율적인 의사소통과 협업, 명확한 업무 책임만이 시스템의 완결성과 프로젝트의 완성을 보장할 수 있다. 아래는 데이터 엔지니어링과 업무적 관계가 있는 직무들이다.

### 7.1. Analysts  
분석가(Analysts)는 빅데이터를 통해 가치 있는 정보를 추출하고 데이터에 대한 인사이트를 도출하는 주요 역할이며, 데이터 분석 및 시각화를 통해 의사 결정에 도움을 주며, 비즈니스 요구 사항을 충족시키기 위한 분석 작업을 수행한다.

### 7.2. Modelers  
모델러(Modelers)는 머신러닝 및 딥러닝 모델을 개발하고 훈련시키는 역할을 담당한다. 또한, 모델러는 데이터를 기반으로 예측 및 분류 모델을 구축하여 비즈니스 문제를 해결하고 최적화하는 데 기여한다.

### 7.3. Infrastructure Specialists
인프라 담당자(Infrastructure Specialists)는 빅데이터 시스템을 구축하고 관리하는 역할을 수행하며, 대규모 데이터 저장, 처리 및 분석을 위한 하드웨어 및 소프트웨어 인프라를 구성하며 시스템의 안정성과 확장성을 관리한다.

### 7.4. Data Engineers
데이터 엔지니어(Data Engineers)는 데이터 수집, 전처리 및 저장을 담당한다. 데이터 엔지니어는 데이터 파이프라인을 구축하고 ETL(Extract, Transform, Load) 작업을 수행하여 데이터의 품질과 가용성을 보장하며, 관련된 하드웨어/소프트웨어에 대한 유지보수를 수행한다.

### 7.5. Business Stakeholders
업무 담당자(Business Stakeholders)는 빅데이터 프로젝트의 비즈니스 요구 사항을 정의하고 프로젝트 목표를 제안한다. 또한, 분석 결과를 이해하여 비즈니스 의사 결정에 활용하는 역할을 한다.

### 7.6. Managers
관리자(Managers)는 프로젝트를 계획하고 리소스를 관리하며 프로젝트 일정을 추적하고 관리한다. 또한 수시로 프로젝트의 과정, 더 나아가 목표와 성과를 모니터링하고 조율한다.

### 7.6. Planners
기획자(Planners)는 빅데이터 프로젝트의 전략 및 로드맵을 개발하고 프로젝트의 방향성을 결정한다.

## 8. Big Data Security
시스템을 유지보수하고 운영하는 업무에서 가장 중요한 것은 바로 보안이다. 보안은 시스템 보안, 데이터 보안, 네트워크 보안, 물리적 보안, 코드 보안, 접근 제어 보안, 전송 보안 등 수많은 영역이 존재하고, 데이터 엔지니어는 이 모든 보안 사항에 대해 고려하여, 시스템을 구축해야 한다. 데이터 보안을 위해 취할 수 있는 방안은 크게 수동적 보안적 조치( Manual Data Security)와 보안과 관련된 소프트웨어를 활용하는 방법으로 구분된다.

### 8.1. Manual Data Security
많은 보안적 요소가 존재하지만, 여기에서는 데이터 보안과 접근 제어 보안에 대해서만 설명한다.

#### 8.1.1. Data Security
데이터 보안은 개인과 기업의 정보 보호를 위한 정책과 기술을 의미한다. 빅데이터에서 데이터 보안의 원칙은 "개인식별이 가능한 어떠한 정보도 수집하지 않는다"이다. 그러나, 개인식별 정보란, 이름, 직업, 성별, 주민등록번호, 주소, 전화번호, 여권번호, 위치 정보 등 매우 다양할 수 있는데, 이러한 개인정보를 수집하지 못한 빅데이터 분석 자체는 의미가 없기 때문에, 대부분의 경우, 비식별화 처리를 하여 적재한다. 이 식별화 처리는 범주화, 삭제, 마스킹(Data masking)이 포함된다.

#### 8.1.2. Access Control Security
접근 제어를 위해선 아이디/패스워드를 인증하는 인증관리자, 인증된 계정의 역할과 권한을 부여하는 권한관리자 기능이 필요하다. 

- 인증관리자(Authentication Manager)는 시스템에 접근하려는 사용자가 자신의 신원을 확인하는 과정을 관리한다. 이를 통해, 사용자는 아이디와 패스워드 또는 다른 인증 수단을 사용하여 자신을 인증할 수 있다. 인증 관리자는 사용자가 올바른 인증 정보를 제공했는지 확인하고, 인증된 사용자에게 접근 권한을 부여한다.
  
- 권한관리자(Authorization Manager)는 인증된 사용자에 대한 역할과 권한을 관리한다. 시스템 내에서 다양한 역할(role)을 정의하고, 각 역할에는 특정 권한(permission)을 할당한다.

### 8.2. Data Security with Software
위 보안적 노력에도 불구하고, 빅데이터 시스템의 접근 제어는 완벽하게 처리하는 데 어려움이 따른다. 특히, 가상 또는 분산 환경에서 다양한 오픈 소스 빅데이터 소프트웨어를 사용하는 경우, 접근 보안 정책을 적용하기 위해 소스 코드 수정이 필요하거나 유료 버전의 솔루션을 구입해야 하기 때문이다. 또한, 대부분의 빅데이터 시스템은 물리적으로 방화벽 안에 위치하여 외부 공격이나 무단 접근
이 원척적으로 불가하며, 데이터는 위에서 설명한 바와 같이 비식별화된 데이터로 저장되어 있기 때문에 대게는 높지 않은 수준이 접근제어 정책을 채택하여 적용한다.

하지만, 금융 분야나 민감한 개인 정보를 다루는 기업 내 빅데이터 시스템의 경우, 엄격한 접근 제어와 보안 준수가 요구 되기 때문에, 이를 해결하기 위해 아파치 녹스 (Apache Knox), 아파치 센트리 (Apache Sentry), 아파치 레인저 (Apache Ranger), 케베로스 (Kerberos) 등을 활용할 수 있다.


### 8.2.1. Data Security with Software


### 8.2.2. Data Security with Software


### 8.2.3. Data Security with Software


### 8.2.4. Data Security with Software


