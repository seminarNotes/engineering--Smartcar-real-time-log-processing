# Framework
최초 작성일 : 2024-01-23  
마지막 수정일 : 2024-01-25

## 0. Overview

아래는 빅데이터를 처리하기 위한 SW/FW이다. 

### 1. Hadoop
#### 1.1. 설명  
Apache Hadoop은 대용량 데이터를 저장하고 처리하기 위한 오픈소스 프레임워크이다. Hadoop은 클러스터로 구성된 컴퓨터에서 대량의 데이터 집합을 분산 처리하는 데 사용됩니다. Hadoop의 핵심은 HDFS(Hadoop Distributed File System)와 MapReduce 프로그래밍 모델로 구성되어 있다.

#### 1.2. 특징  
- **분산 처리**: 데이터와 처리 작업을 여러 서버에 분산하여 처리할 수 있다.
- **확장성**: 서버를 추가함으로써 시스템을 쉽게 확장할 수 있다.
- **고가용성**: 데이터를 여러 노드에 복제하여 하나의 노드에 장애가 발생해도 시스템을 계속 운영할 수 있다.
- **결함 허용성**: 자동으로 데이터를 복제하고 장애가 발생한 노드를 복구한다.

#### 1.3. 구성요소  
- **HDFS (Hadoop Distributed File System)**: 대용량 데이터를 저장하기 위한 분산 파일 시스템.
- **MapReduce**: 대용량 데이터를 병렬로 처리하기 위한 프로그래밍 모델.
- **YARN (Yet Another Resource Negotiator)**: 클러스터 자원 관리 및 작업 스케줄링을 담당.

#### 1.4. 유사 프레임워크  
- Apache Spark
- Apache Flink

### 2. Spark
#### 2.1. 설명  
Apache Spark는 대용량 데이터 처리를 위한 오픈소스 분산 컴퓨팅 시스템이다. Spark는 Hadoop MapReduce와 호환되지만, 반복적인 데이터 흐름을 최적화하기 위해 메모리 내 컴퓨팅과 빠른 처리 속도를 지원하는 점에서 차별화됩니다.

#### 2.2. 특징  
- **고속 처리**: 인메모리 데이터 처리를 통해 빠른 성능을 제공한다.
- **다양한 언어 지원**: Scala, Java, Python, R 등 다양한 프로그래밍 언어를 지원한다.
- **확장성**: 작은 규모의 데이터부터 페타바이트급 데이터까지 처리할 수 있다.
- **다양한 데이터 소스 지원**: HDFS, HBase, Cassandra, Hive 등 다양한 데이터 소스를 지원한다.

#### 2.3. 구성요소  
- **RDD (Resilient Distributed Datasets)**: 변하지 않는 분산된 데이터 컬렉션.
- **Spark SQL**: SQL과 DataFrame을 사용하여 데이터를 쿼리할 수 있는 모듈.
- **Spark Streaming**: 실시간 데이터 스트리밍 처리를 위한 모듈.
- **MLlib**: 머신러닝 알고리즘을 제공하는 모듈.
- **GraphX**: 그래프 처리를 위한 모듈.

#### 2.4. 유사 프레임워크  
- Apache Flink
- Apache Storm

### 3. Hive
#### 3.1. 설명  
Apache Hive는 Hadoop 상에서 데이터 요약, 쿼리, 분석을 위한 데이터 웨어하우스 인프라를 제공하는 프레임워크이다. Hive는 SQL을 사용하여 데이터를 쿼리하는 HiveQL 인터페이스를 제공하므로, SQL에 익숙한 사용자가 Hadoop 데이터를 쉽게 처리할 수 있다.

#### 3.2. 특징  
- **SQL 지원**: HiveQL을 통해 SQL과 유사한 쿼리를 사용할 수 있다.
- **확장성**: Hadoop의 분산 저장 및 처리 기능을 활용하여 대용량 데이터를 처리할 수 있다.
- **다양한 데이터 형식 지원**: Text, Parquet, ORC 등 다양한 데이터 형식을 지원한다.

#### 3.3. 구성요소  
- **Hive Metastore**: 메타데이터를 저장하는 컴포넌트.
- **Hive Driver**: 쿼리 실행 계획을 관리하고 실행하는 컴포넌트.
- **Hive Compiler**: HiveQL을 실행 계획으로 변환하는 컴포넌트.
- **Hive Execution Engine**: 쿼리를 실행하는 컴포넌트.

#### 3.4. 유사 프레임워크  
- Apache Impala
- Presto

### 4. Pig
#### 4.1. 설명  
Apache Pig는 Hadoop 데이터에 대한 고수준 스크립트 플랫폼이다. Pig는 Hadoop의 복잡한 Java MapReduce 프로그램을 대신하여, 간단한 스크립트 언어 Pig Latin을 통해 데이터 분석 작업을 쉽게 할 수 있게 한다.

#### 4.2. 특징  
- **간결한 스크립팅 언어**: Pig Latin을 사용하여 복잡한 데이터 변환을 간단한 단계로 표현할 수 있다.
- **확장성**: Hadoop 인프라를 활용하여 대용량 데이터를 처리할 수 있다.
- **다양한 데이터 소스 지원**: HDFS, HBase 등 다양한 데이터 소스를 지원한다.

#### 4.3. 구성요소  
- **Pig Latin**: 데이터 처리 작업을 위한 스크립팅 언어.
- **Pig Engine**: Pig Latin 스크립트를 실행하는 실행 엔진.
- **UDF (User Defined Functions)**: 사용자가 정의한 함수를 지원하여 데이터 처리를 확장할 수 있다.

#### 4.4. 유사 프레임워크  
- Apache Hive
- Apache Spark


### 5. HBase
#### 5.1. 설명  
HBase는 Apache Hadoop 프로젝트의 일부로 개발된 오픈 소스 분산형 NoSQL 데이터베이스이다. HBase는 대용량 데이터의 저장 및 검색에 특화된 분산형 데이터베이스로, 대규모 데이터를 실시간으로 처리하고 분석하기 위한 목적으로 설계되었습니다.

#### 5.2. 특징  
- **분산 아키텍처**: 여러 서버에 데이터를 분산 저장하며, 데이터를 자동으로 분산하고 복제하여 고가용성을 제공한다.
- **열 지향 데이터베이스**: 데이터를 테이블 형식으로 저장하며, 테이블은 로우(Row)와 열(Column)로 구성됩니다.
- **스캔 및 실시간 쿼리**: 대용량 데이터 세트를 스캔하고 실시간으로 쿼리할 수 있는 기능을 제공한다.

#### 5.3. 구성요소  
- **HMaster**: 클러스터 관리 및 조정.
- **HRegionServer**: 데이터 저장과 처리를 담당하는 서버.
- **HFile**: 실제 데이터 파일.
- **Zookeeper**: 클러스터 조정과 상태 관리.

#### 5.4. 유사 프레임워크  
- Cassandra
- Accumulo

### 6. Flink
#### 6.1. 설명  
Apache Flink은 대규모 분산 데이터 처리를 위한 오픈 소스 스트리밍 프로세스 프레임워크이다. Flink는 빠른 계산과 낮은 지연 시간을 목표로 하며, 배치 및 스트리밍 데이터 처리를 모두 지원한다.

#### 6.2. 특징  
- **스트리밍과 배치 처리**: 실시간 스트리밍 처리와 배치 처리를 모두 지원한다.
- **고성능**: 빠른 처리 속도와 낮은 지연 시간을 제공한다.
- **확장성**: 크고 작은 클러스터 모두에서 확장성 있게 작동한다.

#### 6.3. 구성요소  
- **DataStream API**: 실시간 스트리밍 데이터를 처리하기 위한 API.
- **DataSet API**: 배치 데이터 처리를 위한 API.
- **Table API & SQL**: SQL과 유사한 쿼리 언어를 제공한다.

#### 6.4. 유사 프레임워크  
- Apache Storm
- Apache Spark

### 7. Kafka
#### 7.1. 설명  
Apache Kafka는 대용량 데이터 스트림을 처리하기 위한 분산형 스트리밍 플랫폼이다. Kafka는 데이터 파이프라인을 구축하고, 실시간 데이터 분석을 수행하는 데 사용됩니다.

#### 7.2. 특징  
- **고성능**: 높은 처리량을 제공하며, 대규모 메시지를 신속하게 처리할 수 있다.
- **분산 시스템**: 데이터를 여러 서버에 분산하여 저장하고 처리할 수 있다.
- **내구성**: 데이터를 디스크에 저장하고, 복제를 통해 데이터 손실에 대비한다.

#### 7.3. 구성요소  
- **Producer**: 데이터를 생성하고 Kafka 클러스터에 보내는 컴포넌트.
- **Consumer**: Kafka 클러스터에서 데이터를 읽는 컴포넌트.
- **Broker**: Kafka 클러스터의 서버 노드.

#### 7.4. 유사 프레임워크  
- RabbitMQ
- Apache Pulsar

### 8. Cassandra
#### 8.1. 설명  
Apache Cassandra는 대규모 분산 데이터베이스 관리 시스템이다. Cassandra는 고가용성과 확장성을 제공하며, 많은 기업에서 대규모 데이터를 관리하기 위해 사용됩니다.

#### 8.2. 특징  
- **분산 설계**: 데이터를 여러 노드에 걸쳐 분산 저장한다.
- **확장성**: 데이터와 트래픽 증가에 따라 선형적으로 확장할 수 있다.
- **내구성**: 데이터를 여러 노드에 복제하여 고가용성을 보장한다.

#### 8.3. 구성요소  
- **Node**: 데이터를 저장하는 기본 단위.
- **Cluster**: 여러 노드가 모여 구성된 집합.
- **Keyspace**: 데이터베이스 스키마와 유사한 역할을 하는 최상위 데이터 구조.

#### 8.4. 유사 프레임워크  
- HBase
- MongoDB

### 9. Beam (Apache Beam)
#### 9.1. 설명  
Apache Beam은 배치 및 실시간 데이터 처리를 위한 통합 모델을 제공하는 오픈 소스 프레임워크이다. Beam은 다양한 실행 엔진(Flink, Spark, Google Cloud Dataflow 등) 위에서 동작할 수 있다.

#### 9.2. 특징  
- **모델의 일관성**: 배치 및 스트리밍 데이터 처리에 대해 일관된 프로그래밍 모델을 제공한다.
- **다양한 백엔드 지원**: 여러 실행 엔진에서 동작할 수 있다.
- **휴대성**: 다양한 프로그래밍 언어와 플랫폼에서 작동한다.

#### 9.3. 구성요소  
- **PCollection**: 데이터를 표현하는 핵심 데이터 구조.
- **PTransform**: 데이터 변환을 수행하는 작업.
- **Pipeline**: PTransform의 연속적인 집합을 정의하는 작업 흐름.

#### 9.4. 유사 프레임워크  
- Apache Flink
- Apache Spark

### 10. Kylin (Apache Kylin)
#### 10.1. 설명  
Apache Kylin은 대규모 데이터셋에 대한 OLAP(Online Analytical Processing) 쿼리를 지원하는 분산형 분석 데이터베이스이다. Kylin은 Hadoop 상에서 동작하며, 대규모 데이터에 대해 빠른 쿼리 성능을 제공한다.

#### 10.2. 특징  
- **고성능 OLAP**: 대규모 데이터셋에 대한 빠른 쿼리 응답 시간을 제공한다.
- **확장성**: Hadoop 기반으로, 대용량 데이터를 처리할 수 있는 확장성을 제공한다.
- **다차원 분석**: 다차원 데이터 분석을 위한 강력한 기능을 제공한다.

#### 10.3. 구성요소  
- **Cube**: 데이터를 미리 집계하여 저장하는 데이터 구조.
- **Query Engine**: 사용자의 쿼리에 대한 응답을 생성하는 엔진.
- **Storage Layer**: Cube 데이터를 저장하는 계층.

#### 10.4. 유사 프레임워크  
- Druid
- ClickHouse


### 11. Tez (Apache Tez)
#### 11.1. 설명  
Apache Tez는 복잡한 데이터 처리 작업을 위한 범용, 유연한, 고성능의 데이터 처리 엔진이다. Hadoop YARN 위에서 동작하며, 특히 데이터 처리의 복잡한 DAG(Directed Acyclic Graph)를 효율적으로 실행할 수 있도록 설계되었습니다.

#### 11.2. 특징  
- **효율성**: 복잡한 데이터 처리 작업을 위한 최적화된 경로를 제공한다.
- **유연성**: 사용자 정의 가능한 데이터 처리 컴포넌트를 지원한다.
- **확장성**: 대규모 데이터 처리 작업을 처리할 수 있는 높은 확장성을 제공한다.

#### 11.3. 구성요소  
- **DAG**: 데이터 처리 작업을 DAG로 표현하여 복잡한 작업을 효율적으로 관리한다.
- **Tez API**: 사용자가 Tez를 사용하여 데이터 처리 작업을 쉽게 정의할 수 있도록 돕는 API.
- **YARN Integration**: Hadoop YARN과의 통합을 통해 리소스 관리 및 작업 스케줄링을 지원한다.

#### 11.4. 유사 프레임워크  
- Apache Spark
- Apache Flink

### 12. Storm
#### 12.1. 설명  
Apache Storm은 실시간 데이터 스트림 처리를 위한 분산 컴퓨팅 시스템이다. Storm은 빠른 처리 속도, 확장성, 그리고 내결함성을 제공하며, 실시간 분석, 온라인 머신러닝, 지속적인 컴퓨팅 등 다양한 용도로 사용됩니다.

#### 12.2. 특징  
- **실시간 처리**: 데이터를 실시간으로 처리하고 분석할 수 있다.
- **확장성**: 클러스터에 노드를 추가함으로써 쉽게 확장할 수 있다.
- **내결함성**: 자동으로 장애 노드를 감지하고 복구한다.

#### 12.3. 구성요소  
- **Spout**: 외부 데이터 소스로부터 데이터를 읽어 들이는 컴포넌트.
- **Bolt**: 데이터 처리 및 변환을 담당하는 컴포넌트.
- **Topology**: Spouts와 Bolts를 연결하여 전체 데이터 처리 구조를 정의하는 구성.

#### 12.4. 유사 프레임워크  
- Apache Flink
- Apache Samza

### 13. ZooKeeper
#### 13.1. 설명  
Apache ZooKeeper는 분산 시스템을 위한 중앙 집중식 서비스로, 네이밍, 구성 관리, 동기화, 그룹 서비스 등의 서비스를 제공한다. 분산 애플리케이션의 조정을 위해 설계되었으며, 높은 가용성과 확장성을 제공한다.

#### 13.2. 특징  
- **분산 조정**: 클러스터 내의 노드 간 동기화와 조정을 관리한다.
- **서비스 디스커버리**: 시스템 구성 요소 간 서비스 디스커버리를 지원한다.
- **구성 관리**: 시스템 구성 정보의 중앙 집중식 관리를 제공한다.

#### 13.3. 구성요소  
- **ZNode**: ZooKeeper 데이터 모델의 기본 단위. 데이터와 함께 메타데이터를 저장한다.
- **Leader and Follower**: 클러스터 노드들은 리더와 팔로워로 구성되며, 리더는 클러스터 조정을 담당한다.
- **Watchers**: 클라이언트가 특정 ZNode의 변경사항을 감시할 수 있게 하는 메커니즘.

#### 13.4. 유사 프레임워크  
- etcd
- Consul

### 14. Impala
#### 14.1. 설명  
Apache Impala는 Hadoop 클러스터에서 대규모 데이터를 실시간으로 쿼리할 수 있는 SQL 엔진이다. Impala는 분석 쿼리를 위해 최적화되어 있으며, Hive와 호환되는 메타스토어를 사용한다.

#### 14.2. 특징  
- **실시간 쿼리**: 빠른 쿼리 응답 시간을 제공하여 대용량 데이터에 대해 실시간 분석을 가능하게 한다.
- **SQL 지원**: 기존 SQL 지식을 활용하여 Hadoop 데이터에 접근할 수 있다.
- **확장성**: Hadoop의 분산 저장 및 처리 기능을 활용하여 대용량 데이터를 처리할 수 있다.

#### 14.3. 구성요소  
- **Daemon**: 데이터 노드에서 동작하며, 쿼리 실행을 담당한다.
- **Statestore**: 클러스터 상태 정보를 관리하고, 노드 간 동기화를 담당한다.
- **Catalog Service**: 메타데이터 정보를 관리하고, Impala Daemon과 Hive Metastore 간의 일관성을 유지한다.

#### 14.4. 유사 프레임워크  
- Apache Drill
- Apache Hive


### 15. Pinot (Apache Pinot)
#### 15.1. 설명  
Apache Pinot는 실시간 분석을 위한 분산형 OLAP 데이터 스토어이다. Pinot는 대규모 데이터셋에 대해 실시간 쿼리를 처리할 수 있으며, 사용자는 거의 실시간으로 데이터를 쿼리하고 분석할 수 있다.

#### 15.2. 특징  
- **실시간 데이터 색인**: 데이터를 실시간으로 색인하여 빠른 쿼리 응답 시간을 제공한다.
- **확장성**: 대규모 데이터셋을 처리할 수 있도록 설계되었습니다.
- **고가용성**: 데이터를 복제하고 분산 저장하여 높은 가용성을 보장한다.

#### 15.3. 구성요소  
- **Broker**: 쿼리를 받고 처리 결과를 반환한다.
- **Server**: 데이터를 저장하고 쿼리를 처리한다.
- **Controller**: 클러스터 관리와 세그먼트 할당을 담당한다.

#### 15.4. 유사 프레임워크  
- Druid
- ClickHouse

### 16. Arrow (Apache Arrow)
#### 16.1. 설명  
Apache Arrow는 컬럼 기반의 인메모리 데이터 구조로, 다양한 데이터 시스템 간에 효율적인 데이터 교환을 가능하게 한다. Arrow는 데이터 처리와 분석을 위한 표준 메모리 포맷을 제공하여, 성능을 향상시키고 복잡한 데이터 파이프라인을 간소화한다.

#### 16.2. 특징  
- **메모리 최적화**: 인메모리 데이터 처리를 최적화하여 빠른 데이터 액세스를 지원한다.
- **언어 독립성**: C++, Java, Python 등 다양한 프로그래밍 언어에서 사용할 수 있다.
- **데이터 교환 효율성**: 서로 다른 데이터 처리 시스템 간의 데이터 교환 오버헤드를 최소화한다.

#### 16.3. 구성요소  
- **Arrow Format**: 데이터 구조 및 형식을 정의한다.
- **Arrow Flight**: 고성능 데이터 서비스 프로토콜로, 대규모 데이터 전송을 위한 프레임워크이다.
- **Libraries and Tools**: 다양한 언어와 통합을 지원하는 라이브러리와 도구들.

#### 16.4. 유사 프레임워크  
- Parquet
- ORC

### 17. Avro (Apache Avro)
#### 17.1. 설명  
Apache Avro는 데이터 직렬화 시스템으로, 데이터 구조를 정의하고, 이진 형식으로 데이터를 직렬화 및 역직렬화한다. Avro는 특히 Hadoop 시스템에서 널리 사용되며, 스키마가 데이터와 함께 저장되는 것이 특징이다.

#### 17.2. 특징  
- **스키마 진화**: 스키마 변경 시에도 데이터를 손실 없이 처리할 수 있다.
- **효율적인 직렬화**: 데이터를 컴팩트한 바이너리 형식으로 직렬화한다.
- **언어 독립적**: 다양한 프로그래밍 언어로 구현할 수 있다.

#### 17.3. 구성요소  
- **Avro Data File**: 데이터와 스키마가 함께 저장되는 파일 형식.
- **Avro RPC**: 원격 프로시저 호출을 지원한다.
- **Avro Schema**: 데이터 구조를 정의하는 JSON 형식의 스키마.

#### 17.4. 유사 프레임워크  
- Protocol Buffers
- Thrift

### 18. NiFi (Apache NiFi)
#### 18.1. 설명  
Apache NiFi는 데이터 플로우 자동화와 관리를 위한 시스템이다. NiFi는 데이터 플로우의 설계, 제어, 자동화 및 모니터링을 지원하여, 데이터 전송과 변환 과정을 간소화한다.

#### 18.2. 특징  
- **직관적인 사용자 인터페이스**: 데이터 플로우를 시각적으로 설계하고 관리할 수 있다.
- **확장성**: 소규모에서 대규모 데이터 플로우까지 지원한다.
- **데이터 프로버넌스**: 데이터의 원본, 흐름, 처리 내역을 추적하고 기록한다.

#### 18.3. 구성요소  
- **Processor**: 데이터를 처리하고 변환하는 구성요소.
- **Connection**: 프로세서 간 데이터 플로우를 연결한다.
- **Flow Controller**: 데이터 플로우를 관리하고 조정한다.

#### 18.4. 유사 프레임워크  
- Apache Camel
- Apache Kafka

### 19. Oozie (Apache Oozie)
#### 19.1. 설명  
Apache Oozie는 Hadoop 작업(예: MapReduce, Pig, Hive)의 워크플로우를 관리하고 조정하기 위한 시스템이다. Oozie는 복잡한 데이터 처리 작업을 일련의 작업으로 정의하고, 이들을 순차적 혹은 병렬로 실행할 수 있게 한다.

#### 19.2. 특징  
- **워크플로우 관리**: 다양한 Hadoop 작업을 통합하여 워크플로우를 구성하고 관리한다.
- **스케줄링**: 작업의 실행을 시간이나 데이터 가용성에 따라 예약한다.
- **확장성과 유연성**: 복잡한 데이터 처리 로직을 워크플로우로 설계하여 처리한다.

#### 19.3. 구성요소  
- **Workflow Engine**: 워크플로우의 실행과 생명주기를 관리한다.
- **Coordinator**: 워크플로우의 실행을 시간 혹은 데이터 이벤트에 따라 예약한다.
- **Bundle**: 여러 워크플로우와 코디네이터를 묶어서 관리한다.

#### 19.4. 유사 프레임워크  
- Apache Airflow
- Luigi

### 20. Sqoop (Apache Sqoop)
#### 20.1. 설명  
Apache Sqoop은 SQL 기반의 데이터베이스와 Hadoop HDFS 사이에 대용량 데이터를 효율적으로 전송할 수 있는 도구이다. Sqoop은 데이터를 Hadoop 생태계의 다양한 데이터 스토리지 시스템(예: HDFS, Hive, HBase)과 동기화할 수 있다.

#### 20.2. 특징  
- **효율적인 데이터 전송**: 대용량의 데이터를 RDBMS와 Hadoop 사이에서 빠르고 효율적으로 전송한다.
- **병렬 처리**: 데이터 전송 과정에서 병렬 처리를 지원하여 성능을 최적화한다.
- **다양한 데이터 소스 지원**: 여러 SQL 데이터베이스에 대한 지원을 제공한다.

#### 20.3. 구성요소  
- **Import**: RDBMS에서 Hadoop으로 데이터를 가져옵니다.
- **Export**: Hadoop에서 RDBMS로 데이터를 내보냅니다.
- **Connectors**: 다양한 데이터 소스와의 연결을 지원한다.

#### 20.4. 유사 프레임워크  
- Apache Flume
- Apache Kafka Connect

### 21. Tika (Apache Tika)
#### 21.1. 설명  
Apache Tika는 문서 분석과 메타데이터 추출을 위한 도구이다. Tika는 다양한 파일 포맷(예: PDF, Microsoft Office, HTML)에서 텍스트와 메타데이터를 추출할 수 있다.

#### 21.2. 특징  
- **다양한 파일 포맷 지원**: 수백 가지의 파일 포맷을 지원한다.
- **메타데이터 추출**: 파일로부터 메타데이터를 추출할 수 있다.
- **언어 독립적**: 다양한 프로그래밍 언어에서 사용할 수 있다.

#### 21.3. 구성요소  
- **Parser**: 다양한 파일 포맷을 파싱하여 텍스트와 메타데이터를 추출한다.
- **Detector**: 파일 포맷을 감지한다.
- **Language Identification**: 텍스트의 언어를 식별한다.

#### 21.4. 유사 프레임워크  
- Apache PDFBox
- Apache POI

### 22. Mahout (Apache Mahout)
#### 22.1. 설명  
Apache Mahout은 머신러닝 알고리즘을 구현하고 대규모 데이터셋에서 이러한 알고리즘을 실행하기 위한 스케일러블한 라이브러리이다. Mahout은 분류, 클러스터링, 추천 시스템 등 다양한 머신러닝 기법을 지원한다.

#### 22.2. 특징  
- **다양한 머신러닝 알고리즘**: 분류, 클러스터링, 추천 시스템 등을 지원한다.
- **확장성**: Hadoop을 이용하여 대용량 데이터셋을 처리할 수 있다.
- **프로그래밍 언어 독립성**: Java 및 기타 언어를 통해 사용할 수 있다.

#### 22.3. 구성요소  
- **Algorithms**: 다양한 머신러닝 알고리즘을 제공한다.
- **Math Library**: 수학적 연산을 위한 라이브러리를 제공한다.
- **Integration**: Hadoop과의 통합을 지원하여 대규모 데이터 처리를 가능하게 한다.

#### 22.4. 유사 프레임워크  
- MLlib (Apache Spark)
- Scikit-learn

### 23. Drill (Apache Drill)
#### 23.1. 설명  
Apache Drill은 대규모 분산 데이터 저장 시스템에서 구조화되지 않은 또는 반구조화된 데이터를 위한 SQL 쿼리 엔진이다. Drill은 복잡한 데이터 분석을 위해 다양한 데이터 소스를 통합하고, 사용하기 쉬운 SQL 인터페이스를 제공한다.

#### 23.2. 특징  
- **다양한 데이터 소스 지원**: HDFS, NoSQL, 클라우드 저장소 등 다양한 데이터 소스를 지원한다.
- **스키마 온-더-플라이**: 데이터를 쿼리할 때 스키마를 동적으로 해석한다.
- **확장성과 성능**: 대규모 데이터셋에 대해 빠른 쿼리 성능을 제공한다.

#### 23.3. 구성요소  
- **Query Engine**: SQL 쿼리를 실행하는 엔진.
- **Storage Plugin**: 다양한 데이터 소스와의 연결을 관리한다.
- **Execution Engine**: 쿼리 계획을 실행하고 결과를 처리한다.

#### 23.4. 유사 프레임워크  
- Apache Hive
- Apache Impala

### 24. Calcite (Apache Calcite)
#### 24.1. 설명  
Apache Calcite은 데이터베이스와 스트리밍 시스템을 위한 독립적인 SQL 쿼리 엔진 및 프레임워크이다. Calcite는 다양한 데이터 소스에 대해 통합된 SQL 인터페이스를 제공하며, 쿼리 최적화 및 실행 계획을 생성한다.

#### 24.2. 특징  
- **플러그형 아키텍처**: 다양한 데이터 스토리지 및 처리 시스템에 쉽게 통합할 수 있다.
- **고급 쿼리 최적화**: 여러 최적화 규칙과 비용 기반 최적화를 제공한다.
- **다양한 쿼리 언어 지원**: SQL 뿐만 아니라 다양한 쿼리 언어를 지원한다.

#### 24.3. 구성요소  
- **SQL Parser**: SQL 쿼리를 파싱한다.
- **Query Optimizer**: 쿼리를 최적화하고 실행 계획을 생성한다.
- **Query Executor**: 최적화된 쿼리를 실행한다.

#### 24.4. 유사 프레임워크  
- Apache Drill
- Apache Hive

### 25. Flume (Apache Flume)
#### 25.1. 설명  
Apache Flume은 대규모 로그 데이터를 수집하고, 집계하며, Hadoop 같은 데이터 저장소로 이동시키기 위한 분산 시스템이다. Flume은 데이터 생성 소스에서 HDFS, HBase 등의 목적지까지 안정적으로 데이터를 전송한다.

#### 25.2. 특징  
- **데이터 수집**: 다양한 소스로부터 로그 데이터와 이벤트를 수집한다.
- **데이터 집계**: 수집된 데이터를 집계하고 효율적으로 처리한다.
- **데이터 전송**: 데이터를 안전하고 신뢰성 있게 Hadoop 저장소로 전송한다.

#### 25.3. 구성요소  
- **Agent**: 데이터를 수집하고 처리하는 독립적인 프로세스.
- **Source**: 데이터를 수집하는 시작점.
- **Channel**: 데이터를 임시로 저장하는 버퍼.
- **Sink**: 데이터를 최종 목적지로 전송하는 구성요소.

#### 25.4. 유사 프레임워크  
- Apache Kafka
- Logstash

### 26. Superset
#### 26.1. 설명  
Superset은 데이터 탐색 및 시각화를 위한 오픈 소스 비즈니스 인텔리전스 도구이다. 사용자는 Superset을 사용하여 데이터를 쉽게 탐색하고, 대화형 대시보드를 생성하며, 다양한 시각화를 통해 데이터를 분석할 수 있다.

#### 26.2. 특징  
- **다양한 데이터 소스 지원**: SQL 데이터베이스, Druid, Google BigQuery 등 다양한 데이터 소스를 지원한다.
- **강력한 시각화 도구**: 다양한 시각화 옵션과 사용자 정의 대시보드를 제공한다.
- **사용자 친화적 인터페이스**: 직관적인 사용자 인터페이스를 통해 데이터 탐색과 시각화를 쉽게 수행할 수 있다.

#### 26.3. 구성요소  
- **Dashboard**: 다양한 시각화를 통합하여 보여주는 대시보드.
- **Chart**: 데이터를 시각화하는 다양한 차트 옵션.
- **SQL Lab**: SQL 쿼리를 작성하고 데이터를 탐색할 수 있는 도구.

#### 26.4. 유사 프레임워크  
- Grafana
- Tableau

### 27. Beats
#### 27.1. 설명  
Beats는 Elastic Stack의 경량 데이터 수집기로, 다양한 유형의 데이터를 수집하여 Logstash 또는 Elasticsearch로 전송할 수 있다. Beats는 시스템 로그, 네트워크 트래픽, 클라우드 서비스 데이터 등 다양한 데이터 소스에서 데이터를 수집할 수 있다.

#### 27.2. 특징  
- **경량성**: 시스템 자원을 많이 사용하지 않으면서 데이터를 효율적으로 수집한다.
- **모듈성**: Filebeat, Metricbeat, Packetbeat 등 특정 데이터 유형에 특화된 다양한 Beats가 있다.
- **유연성**: 데이터를 Logstash로 전송하여 추가 처리를 할 수도 있고, 직접 Elasticsearch로 전송할 수도 있다.

#### 27.3. 구성요소  
- **Filebeat**: 로그 파일을 수집하기 위한 Beats.
- **Metricbeat**: 시스템, 서비스, 컨테이너의 메트릭을 수집하기 위한 Beats.
- **Packetbeat**: 네트워크 패킷 데이터를 수집하기 위한 Beats.

#### 27.4. 유사 프레임워크  
- Fluentd
- Logstash

### 28. Logstash
#### 28.1. 설명  
Logstash는 데이터 처리 파이프라인을 구축하는 데 사용되는 서버 사이드 데이터 처리 파이프라인 도구이다. 로그, 이벤트 데이터 등을 수집, 변환, 저장하는 데 사용되며, Elastic Stack의 주요 구성 요소 중 하나이다.

#### 28.2. 특징  
- **다양한 플러그인 지원**: 입력, 필터, 출력 플러그인을 통해 다양한 소스의 데이터를 수집하고 변환할 수 있다.
- **강력한 데이터 처리**: 데이터 변환, 정제, 풍부화 등 복잡한 데이터 처리 작업을 수행할 수 있다.
- **확장성**: 대량의 데이터를 처리할 수 있도록 설계되었습니다.

#### 28.3. 구성요소  
- **Input Plugins**: 다양한 소스로부터 데이터를 수집한다.
- **Filter Plugins**: 데이터를 변환하고 풍부화하는 데 사용됩니다.
- **Output Plugins**: 처리된 데이터를 다양한 목적지로 전송한다.

#### 28.4. 유사 프레임워크  
- Fluentd
- Apache NiFi

### 29. Elasticsearch
#### 29.1. 설명  
Elasticsearch는 높은 확장성을 갖춘 분산형 검색 및 분석 엔진이다. Elasticsearch는 복잡한 검색 기능, 실시간 분석, 그리고 텍스트, 숫자, 지리적 위치, 구조화된 및 비구조화된 데이터 등 다양한 유형의 데이터를 처리할 수 있다.

#### 29.2. 특징  
- **풀 텍스트 검색**: 강력한 풀 텍스트 검색 기능을 지원한다.
- **분산 시스템**: 데이터를 여러 노드에 분산하여 저장하고 처리한다.
- **실시간 분석**: 데이터를 실시간으로 색인하고 검색할 수 있다.

#### 29.3. 구성요소  
- **Index**: 동일한 특성을 갖는 문서들의 집합이다.
- **Document**: 데이터의 기본 단위로, JSON 형식으로 저장됩니다.
- **Node & Cluster**: 단일 노드 또는 여러 노드가 모여 구성된 클러스터로 데이터를 저장하고 처리한다.

#### 29.4. 유사 프레임워크  
- Apache Solr
- Sphinx

### 30. Kibana
#### 30.1. 설명  
Kibana는 Elasticsearch 데이터를 시각화하고 탐색하는 데 사용되는 오픈 소스 데이터 시각화 플랫폼이다. Kibana를 사용하면 데이터에 대한 인사이트를 얻기 위해 차트, 테이블, 지도 등 다양한 방식으로 데이터를 시각화할 수 있다.

#### 30.2. 특징  
- **다양한 시각화 옵션**: 막대 차트, 선 차트, 파이 차트, 지도 등 다양한 시각화 옵션을 제공한다.
- **대시보드 생성**: 여러 시각화를 결합하여 대시보드를 생성할 수 있다.
- **데이터 탐색**: Elasticsearch에서 데이터를 탐색하고 상세 분석을 수행할 수 있다.

#### 30.3. 구성요소  
- **Visualizations**: 데이터를 시각화하는 다양한 방법.
- **Dashboard**: 여러 시각화를 결합한 사용자 정의 가능한 대시보드.
- **Discover**: 데이터를 탐색하고 검색할 수 있는 인터페이스.

#### 30.4. 유사 프레임워크  
- Grafana
- Tableau



